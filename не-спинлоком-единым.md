# Не spinlock'ом единым?

_Сказ о spinlock'ах, atomic'ах и системах распределенных._

## Да будет spinlock

[Летописи Линукса](https://docs.kernel.org/locking/spinlocks.html#lesson-1-spin-locks) не спроста начинаются словами: 
> The most basic primitive for locking is spinlock.

И это правда: проще просто некуда! В своей сути `spinlock` это (не)обыкновенный `bool`, где `false` означает "свободно", а `true` — кто бы мог подумать! — "занято". Идея в том, чтобы крутиться в цикле `while` до тех пор, покуда "занятно", а когда станет "свободно" — первым делом пометить, что теперь опять "занято" и начать выполнение критической секции, по завершению которой не забыть вернуть этот `spinlock` в состояние "свободно":

```C
while (is_busy); // спокойствие, только спокойствие...

// цикл выше закончился, следовательно должно было стать "сводобно",
// поэтому сперва надо вернуть "занято"
is_busy = true; // ...вот теперь "занято" и можно делюгу делать

// К Р И Т И Ч Е С К А Я   С Е К Ц И Я
// дела делаются,
// делаются,
// еще немного...
// ...сделались

is_busy = false; // раз уж делать больше нечего, то снова "свободно"
```

Проще быть не могло хотя бы потому что:

1. `while` совершенно бессовестным образом [сжигает драгоценное время CPU](https://en.wikipedia.org/wiki/NOP_(code)), нагревая холодный и безразличный Космос. Никакого хитроумного способа не жечь CPU быть не могло поскольку `spinlock` предназначен для использования в [kernelspac'е](https://linux-kernel-labs.github.io/refs/heads/master/lectures/intro.html#user-vs-kernel) и далеко не во всех контекстах — например, обработка прерываний — мог бы полагаться на возможность "уснуть" [^1].

2. Меньше, чем двумя состояниями обойтись, очевидно, не выйдет. Потратить меньше памяти, чем фактически занимает `bool` на данной конкретной архитектуре (в подавляющем большинстве современных случаев это аж один байт, хотя в теории достаточно ровно одного бита) — тоже не выйдет.

3. Нет и не предвидится никакой управляемой отмены (насильно перестать пытаться захватить `spinlock`). Не то, чтобы ее прям сложно добавить, но, строго говоря, она (для задачи блокировки в ее отрыве от практики) не необходима [^2].

И все бы хорошо, и жили бы они долго и счастливо, да не судьба. Код выше — не работает. Сейчас будем чинить.

## Да будет spinlock, который работает

А вот тут все мгновенно усложняется. Потому что на самом деле, чтобы написать работающий во всех случаях `spinlock`, нужно решить как минимум три проблемы:

1. Надо сделать так, чтобы разные CPU всегда видели `is_busy` одинаково. Не должно быть так, чтобы одно ядро уже записало туда `true`, в то время как другое соседнее ядро по-прежнему продолжает считывать `false` [^3].

2. А еще надо уговорить компилятор не наоптимизировать исходный код до того, что он перестанет работать, но при этом не потерять принципиальную возможность применять [весь спектр оптимизаций](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html) в других местах.

3. Наконец, самое сложное: надо решить первую проблему еще и с учетом того немаловажного факта, что современные CPU активно переставляют порядок выполняемых инструкций на бегу, в процессе исполнения. То есть, фраза "чтобы разные CPU _всегда_ видели `is_busy` одинаково" предполагает настоящее "всегда" — такое "всегда", которое гарантированно распространяется и на [многочисленные аппаратные финты](http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf) тоже. 

Наивное "решение" с незамысловатым `bool` уязвимо как минимум по второму и третьему пунктам [^4]: например, комплиятор имеет полное право поднять `is_busy = false` повыше и выполнить эту запись до того как завершены (или даже начаты!) некоторые записи из критической секции. А если не компилятор с его `-O3`, то такое может случиться в процессе непосредственного выполнения на CPU. Благо, в [C11](https://www.iso.org/standard/57853.html) появился достославный [stdatomic.h](https://en.cppreference.com/w/c/atomic), правильное использование которого дает изящное решение всех трех проблем разом.

Выглядит оно так:

```C
#include <stdbool.h>
#include <stdatomic.h>

struct spinlock {
      // ...теперь `_Atomic`
      _Atomic bool is_busy;
};

void lock(struct spinlock *sl) {
      while (atomic_exchange(&sl->is_busy, true))
            /* nop, nop, noooooop */ 
            ; 
}

void unlock(struct spinlock *sl) {
      atomic_store(&sl->is_busy, false);
}
```

Сразу оговорюсь, что огромное количество современных архитектур предлагают более разумную альтернативу инструкции `nop`: скажем, ["pause" на x86](https://www.felixcloutier.com/x86/pause.html). Сейчас это не важно, хотя на деле, безусловно, было бы полезно.

Код выше по крайней мере работает — в смысле, он действительно _гарантирует_, что только один из множества соперничающих потоков завершит `lock`, в то время как все остальные останутся висеть в цикле ожидания. Код выше хоть как-нибудь, да решает все три вышеназванные проблемы. Уже неплохо. 

Дело в том, что компилятор не шибко много знает о том, как и в первую очередь на скольки и каким образом синхронизированных потоках будет запускаться (компилируемый им) код. Отсюда следует единственно возможный выбор на этапе компиляции: предполагать, что программа будет выполняться в однопоточной среде и не думать о плохом. В однопоточной программе синхронизировать нечего, а оптимизации и приколы времени выполнения на CPU — безвредны по определению (ведь их проектировали с прицелом на то, чтобы сохранять семантику). Поэтому компилятор keeps it simple stupid и выплевывает байткод, который строго про однопоточность.

Как жаль, что время однопоточных программ безнадежно ушло. Получается, программист заведомо знает, что "именно вот этот" кусок кода задуман для исполнения многими потоками одновременно, на разных ядрах — по-настоящему параллельно... знает, но до `C11` не умел этого сообщить компилятору. Как следствие, последний не умел скомпилировать программу так, чтобы она корректно работала на всех устройствах [^5]. А теперь, вот, умеет. Для этого тщательно избранные участки памяти аннотируются как `_Atomic` и далее еще важно неукоснительно соблюдать магический ритуал `atomic_*` чтений и записей. Компилятор увидит аннотацию и поймет, что от него требуется решить три названные проблемы — и решит их, выдав байткод, который в процессе выполнения [запретит CPU самоволку](https://en.wikipedia.org/wiki/Memory_barrier).

Для дальнейшего взаимопонимания поясню "как оно работает":

1. Если `struct spinlock` передавать не по ссылке, а копировать всю структуру целиком, то копироваться будет и `_Atomic bool is_busy`, что в свою очередь приведет к полнейшей рас-синхронизации ведь потоки будут читать и писать в разные секции необщей для них памяти.

2. `atomic_exchange`, как сказано в документации, возвращает прошлое значение (то, которое было до этой записи). Поскольку это `bool`, то вариантов всегда два: 

      * Было `true` и стало `true`. Это означает, что `spinlock` уже был занят и остался таковым, т.к. в результате возвращения `true` цикл ожидания продолжится и подлинный владелец этого `spinlock`'а продолжит выполнять свою работу бесперебойно. Красиво? Красиво.

      * Было `false` и стало `true`. То есть: было "свободно" и стало "занято". Именно этот случай означает, что данный конкретный поток выиграл борьбу за `spinlock` и теперь является его подлинным владельцем. Поскольку было `false`, то именно это и вернется в результате выполнения `atomic_exchange(&sl->is_busy, true)`, что приведет к прекращению цикла ожидания и завершению самой функции. Красиво? Красиво.

3. `atomic_store` переводит `spinlock` в состояние "свободно". Опять же, могло быть лишь два случая и в худшем из них и без того "свободный" `spinlock` еще раз помечается как "свободный" — хуже от этого никому сделаться не может. Красиво? Пожалуй, да.

## Да будет spinlock, который работает быстрее

А если подумать, чего вообще разумно требовать от такого рода блокировки? 

Так-то теория распределенных систем (ТРС) отнюдь не стоит на месте. Люди не только научились математически строго формулировать свои требования к распределенным системам [^6], но и классифицировать эти требования.

Основных классов таких требований всего два:

* safety — про все плохое, что не должно случаться никогда; про инварианты и гарантии;

* liveness — про все хорошее,а что должно в итоге должно случаться; про полезные и нужные события, ради совершения которых система и создавалась.

Применительно к `spinlock`у, safety, например, может звучать так: 

> В любой момент жизни `spinlock`'а им либо никто не владеет и тогда он в состоянии "свободен", либо им владеет строго один поток и тогда он в состоянии "занят".

Формулировка выше как раз предотвращает плохое — в частности, когда несколько разных потоков убеждены, что владеют `spinlock`'ом самолично, или когда кто-то де-факто владеет `spinlock`'ом, который при этом не "занят", или когда никто `spinlock`'ом и не владеет, а он все равно почему-то "занят". Такого быть не должно _никогда_ [^7] — ни в какой момент времени, начиная с конструкции и заканчивая деструкцией этого примитивного примитива синхронизации.

А что liveness? Ну, например так:

> Каждый поток, который попытался завладеть `spinlock`'ом, в итоге завладеет им.

Эта формулировка не уточняет и не может уточнить как много времени пройдет прежде, чем конкурирующий поток все таки добьется своего. Почему? Потому что эти формулировки в принципе не оперируют временем как таковым, а только лишь _событиями_ и _причинно-следственными порядками между ними_. Тем не менее, эта формулировка предельно четко требует, чтобы не было ни одного потока, который бесконечно долго пытался выиграть и не выигрывал. В каком-то смысле, это требование честности, равноправности, справедливого разделения процессорного времени между игроками-потоками.

Так вот, safety-требование реализация поверх `stdatomic.h` выше удовлетвоярет полностью. А вот с liveness... Хьюстон, у нас проблема! `while (atomic_exchange(&sl->is_busy, true))` ровно ничего никому не обещает, кроме как "если надо, я буду ждать _бесконечно долго_". _Бесконечно_ долгое ожидание своей очереди как раз и запрещается. На самом деле ситуация еще хуже: может ведь произойти еще и такое, что некоторый поток `T` успешно захватит `spinlock`, а потом помрет... помрет, по ошибке или с криком "так не достанься же ты никому!" не выполнив `unlock`. В этом случае ни один из оставишся в игре потоков уже никогда не сможет завершить свой вызов `lock`. GAME OVER. Печалька.

Я веду к тому, что принятый подход к решению с точки зрения liveness — принципиально ущербен и исправить его, не изменив концепцию, нельзя [^8]. Однако можно минимизировать возможный ущерб. Этим и займемся.

Чтобы понять как сделать `spinlock` быстрее, нужно понять почему он медленен. По задумке некоторое ненулевое множество потоков спорят за право первым выйти из уже ни раз упоминавшегося цикла ожидания: `while (atomic_exchange(&sl->is_busy, true))`. Спорят как? Пытаются _атомарно_ поймать момент, когда было `sl->is_busy == false` и _записать_ туда `true`. За-пи-са-ть, понимаете?

Запись от чтения отличается в первую очередь тем, что два конкурентных чтения это нормально, а вот две конкурентных записи — к беде. Вот, например, из `C++11`:

> When an evaluation of an expression writes to a memory location and another evaluation reads or modifies the same memory location, the expressions are said to conflict.

То есть, по этому определению даже случай, когда чтение пересеклось с записью — уже haram под страхом [data race](https://en.cppreference.com/w/cpp/language/memory_model). А по поводу программ с data race C++ (и не только он!) нам ничего, кроме undefined behavior, не обещает.

Тут полезно вспомнить, что у каждого CPU есть свой cache, состоящий из cache lin'ов, длиной примерно по 64 байта. Разумеется, что `&sl->is_busy` всегда попадает в тот или иной cache line, ведь весь смысл синхронизации в том, что некоторая _общая_ память для разных потоков _атомарно_ изменила свое состояние (исключая data race, ага). И такой cache line будет у каждого из задействованных CPU. Следовательно, по протоколам cache coherency, перед каждой записью на каждой итерации цикла ожидания каждый поток на аппаратном уровне должен заполучить write-права, таким образом фактически приводя к инвалидации соответствующих cache lin'ов у всех остальных CPU вокруг себя. Каждый, каждого, каждая... не слишком ли будет?!

Слишком, еще как слишком! Поэтому и тормознутый такой у нас вышел `spinlock`. Работать — работает, но непростительно неэффективно. Зная первопричину, ее легко (почти полностью) устранить:

```C
void lock(struct spinlock *sl) {
      while (atomic_exchange(&sl->is_busy, true))
            while (atomic_load(&sl->is_busy))
                  /* nop, nop, noooooop */ 
                  ;
            /* nop, nop, noooooop */ 
            ; 
}
```

Конечно же `atomic_load` никакой записи никуда не совершает, а значит никакой синхронизации cach'а между CPU не предполагает. Вложенный цикл _более эффективного_ ожидания `while (atomic_load(&sl->is_busy))` концептуально делает то же самое, но без пин-понга злополучного cache lin'а на каждой итерации между всеми потоками. Стало не только красиво, но и гораздо производительнее, чем прежде. Cледовательно — быстрее завершается сам акт синхронизации и больше ресурсов остается для полезной работы (a-ka "критическая секция"). Улучшает ли это liveness? Пожалуй, да, хотя высчитать конркетную величину, на которую стало лучше — не представляется возможным. Интуиция такая: 

> Предполагаем, что все CPU на аппаратном уровне, как и на уровне ОС, совершенно равноправны. Значит, потоки, ими исполняемые, тоже равноправны в своих возможностях. Почему liveness могло нарушаться? Лишь потому что некоторый поток `T`, принадлежащий некоторому CPU `C`, бесконечно долго проигрывал в борьбе за некоторый `spinlock`. Предполагая, что речь идет о достаточно продолжительном эксперименте, получается, что _бесконечно долгое время, а значит — никогда_ `C` не мог выполнить `atomic_exchange` пержде своих соседей-конкурентов. Напрашивается вывод об аппаратной неисправности, что противоречит изначальному предположению.

## А можно еще быстрее? 

![Мiжна!](не-спинлоком-единым-1.jpg "Мiжна!")

Вернемся в священный храм `C11` и прочтем там древние письмена, начертанные на скрижалях бесконечности:

```C
enum memory_order {
    memory_order_relaxed,
    memory_order_consume,
    memory_order_acquire,
    memory_order_release,
    memory_order_acq_rel,
    memory_order_seq_cst
};
```

`memory_order`'ы это что-то вроде форсажа на истребителях — надо знать и уметь, иначе феерично убьешься. Забегая наперед, memory order friendly код мог бы вглядеть так:

```C
#include <stdbool.h>
#include <stdatomic.h>

// сама `struct spinlock` не изменилась
struct spinlock {
      _Atomic bool is_busy;
};

void lock(struct spinlock *sl) {
      while (atomic_exchange_explicit(&sl->is_busy, true, /* магия */ memory_order_acquire))
            while (atomic_load_explicit(&sl->is_busy, /* магия */ memory_order_relaxed))
                  /* nop, nop, noooooop */ 
                  ;
            /* nop, nop, noooooop */ 
            ; 
}

void unlock(struct spinlock *sl) {
      atomic_store_explicit(&sl->is_busy, false, /* магия */ memory_order_release);
}
```

Как написано в документации, если явный `memory_order` не указан, то `memory_order_seq_cst` используется по умолчанию. "seq cst" это сокращенное "sequential consistency" — очередной научный термин [1979 года рождения](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/How-to-Make-a-Multiprocessor-Computer-That-Correctly-Executes-Multiprocess-Programs.pdf), с тех пор легший в основу инженерии распределенных систем всех микро- и макро-уровней. Leslie Lamport, автор статьи, дает такое определение:

> … the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.

Главное: 

* Есть "the order specified by its program". Это порядок инструкций как они написаны программистом в тексте программы. Он всегда один — тот, который определил сам программист.

* Есть "execution order". Это то, как программа по факту выполнилась после того как ее выоптимизировал компилятор и переосмыслил бездушный кремний, пытающийся казаться умным и одухотворенным. Это — фактический порядок выполненных инструкций на всех CPU. Он не может быть "_the_ execution order" потому что таких порядков сосуществует много — в общем случае, у каждого CPU он будет свой. Поэтому никаких _the_.

* Есть "some sequential order". Это какой-то произвольный порядок тех же самых инструкций как если бы они все выполнялись строго на одном CPU и на одном его потоке, а не на многих CPU, выполняющих много потоков. Поэтому "sequential". Это вообще вымышленная сущность, в физическом мире не существующая; в общем случае ничего в таком порядке не выполнялось в силу природной параллельности происходящего. Этот порядок есть попытка смоделировать произошедшее в сложном многомерном параллельном мире в терминах просто одномерного последовательного и скучного мирка.

Пересказываю определение: "sequential consistency" это когда фактический порядок выполнения на разных CPU _можно объяснить_ как если бы все это произошло на воображаемом одном CPU и на воображаемом единственном его потоке, причем с точки зрения _каждого_ потока был соблюден тот _единственный_ порядок чтений и записей, который программист запечатлел в исходном коде для этого потока. Равноправное определение: "sequential consistency" это когда есть общий глобальный порядок выполнения, не противоречащий ни одному локальному порядку. Еще одно равноправное определение: "sequential consistency" это когда между любыми двумя выполнеными инструкциями `i1` и `i2` выполняется следующее утверждение: либо `i1` случилось прежде `i2`, либо `i2` случилось прежде `i1`.

Я не настолько наивен, чтобы предполагать, что это сложное понятие можно объяснить за пару абзацев нематематического текста. Да я и не пытаюсь этого сделать, если честно. Тем более, что мой пересказ неполный, неточный и вообще как его Земля носит?! Вон, [Роман Липовский](https://www.youtube.com/watch?v=aqYhCXExHBU) пытался сделать лучше. [Федор Пикус](https://www.youtube.com/watch?v=ZQFzMfHIxng) тоже пытался. Как по мне, в целом у них хорошо получилось, но если их объяснения не заходит, то термин "sequential consistency" вообще-то легко googl'ится. В конце-концов, прочтите оригинальную статью отца-основателя Leslie или его апостолов! Однако с прискорбием сообщаю, что без понимания, хотя бы примерного, что есть "sequential consistency" в распределенных системах и толковом multithreading'е делать просто нечего. Когда же оно наконец-то нисходит, то следующий вывод становится до смешного очевидным:

> `memory_order_seq_cst` — самое строгое, что можно себе представить; на практике в большинстве случаев такая сильная (и дорогая!) гарантия — избыточна.

В частности, она не нужна в `spinlock`'е выше. Потому что весь смысл критической секции в том, чтобы _отделить_ инструкции А) до критической секции от инструкций Б) ее самой и В) инструкций после нее так, чтобы А), Б) и В) не пересекались (иначе критическая секция внезапно перестанет быть таковой), но при этом чтобы не терять оптимизации _внутри, а не между ними_. Именно для такого случая и предназначен тандем `memory_order_acquire` и `memory_order_release`, благодаря которому можно утвердить нерушимую границу критической секции сверху и снизу, но не более того — то есть, не требуя глобального линейного порядка исполнения А), Б) и В), который бы повлек деградацию быстродействия [^9].

## Мораль сей басни

На самом деле это — своеобразное приглашение. Приглашение в первую очередь задавать вопросы, эксперементировать и разбираться самостоятельно. Учиться _быть инженером_.

И приглашение не столько в мир ЭВМ, сколько в мир распределенных систем, который почему-то пронизан самоподобием: основополагающие действующие силы микроуровня властны так же и на макроуровне.

Например, сердцевина легендарного [Zookeeper'а](https://github.com/apache/zookeeper) — [ZAB, он же **Z**ookeeper **A**tomic **B**roadcast](https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab) — в некотором до математической строгости конкретном смысле есть отражение `stdatomic.h` в макромир. Нельзя обойти молчанием и [уровни изоляции того же PostgreSQL](https://www.postgresql.org/docs/current/transaction-iso.html). А еще без понимания того, что выше сказано и части того, что осталось в умолчаниях, нельзя даже осознать что написано в документации на [Cassandr'у](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlAboutDataConsistency.html#Linearizableconsistency).

Так что думайте сами, решайте сами.

[^1]: Иначе deadlock'ов не оберешься да и в целом для засыпания нужен полновесный планировщик и что-то вроде [pthread_cond_wait](https://man7.org/linux/man-pages/man3/pthread_cond_wait.3p.html).

[^2]: На практике же без возможности остановить блокирующую операцию извне редко когда получается сделать что-то надежное: немалая доля эпичных крахов в распределенных системах связана с тем, что все или значительная часть доступных потоков тем или иным способом заблокировались (в нашем случае, в попытке взять `spinlock`, который сейчас в чьем-то владении и этот кто-то никогда не вернет `spinlock` в состояние "свободно" — например, потому что этот кто-то помер из-за bug'а в последнем releas'е) и ждут чего-то, что никогда так и не настанет. [Принудительная отмена](https://vorpus.org/blog/timeouts-and-cancellation-for-humans) отлично решает эту проблему: ожидание не продлится дольше допущенного и в итоге, рано или поздно, поток все же откажется от блокирующей его операции, вернется к жизни и продолжит делать что-то полезное.

[^3]: Так может случиться, если забыть синхронизировать содержание их cache'й. Если это утверждение неясно, то читать дальше пока что рановато.

[^4]: В подавляющем большинстве современных архитектур cache coherency обеспечивается аппаратно и явно синхронизировать состояние cahc'ей не надо.

[^5]: Другими словами, до `C11` никакой кросс-платформенности в мире многопоточной разработки попросту не существовало.

[^6]: Что открывает прямую дорогу к программной верификации этих требований. Например, [ужинающие философы](https://github.com/tlaplus/Examples/tree/master/specifications/DiningPhilosophers) точно смогут благополучно закончить ужин — проверено с помощью TLA+.

[^7]: Математики называют подобного рода свойства "инвариантами".

[^8]: Между прочим, как раз поэтому в Linux и завезли заморского зверя по имени [ticket spinlock](https://lwn.net/Articles/267968) — чтобы все по науке было и safety, и liveness чтоб мирно сосуществовали.

[^9]: Мера деградации зависит от платформы. Например, на x86 разницы толком не почувствовать, а на ARM'ах она будет измеряться порядками.